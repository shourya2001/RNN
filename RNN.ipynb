{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVCKsKZRQUqD"
      },
      "source": [
        "# RNNs\n",
        "\n",
        "In this section, you will implement a simple Recurrent Neural Network (RNN) from scratch to perform sequence prediction using the IMDb movie reviews dataset.\n",
        "\n",
        "![RNN.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChCCwjXNZL_m"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "from gensim.models import FastText, KeyedVectors\n",
        "from tqdm import tqdm\n",
        "\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXz0kJe_0WEO"
      },
      "source": [
        "#### Load and Prepare Data\n",
        "\n",
        "Next, we load the IMDb dataset and prepare it for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gSq5PBFaOXN"
      },
      "outputs": [],
      "source": [
        "vocab_size = 1000\n",
        "maxlen = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLL67eJtaPMK"
      },
      "outputs": [],
      "source": [
        "(x_train, _), (x_test, _) = imdb.load_data(num_words=vocab_size + 3, maxlen=maxlen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mycpHxZP0RW2"
      },
      "outputs": [],
      "source": [
        "# Create a one-hot encoded representation of the training data\n",
        "x_train_one_hot_encoded = np.zeros((x_train.shape[0], maxlen, vocab_size))\n",
        "\n",
        "# Iterate over each sample and each word index to set the one-hot value to 1\n",
        "for i in range(x_train.shape[0]):\n",
        "    for j in range(len(x_train[i])):\n",
        "        idx = x_train[i][j] - 3\n",
        "        if idx < 0:\n",
        "          idx = 0\n",
        "        x_train_one_hot_encoded[i, j, idx] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwxWAb9TaQI5"
      },
      "outputs": [],
      "source": [
        "# Decode the dataset back into words to train Word2Vec\n",
        "word_index = imdb.get_word_index()\n",
        "index_word = {v: k for k, v in word_index.items() if v < vocab_size}\n",
        "\n",
        "x_train_words = [[index_word.get(idx-3, \"unk\") for idx in sequence] for sequence in x_train]\n",
        "x_test_words = [[index_word.get(idx-3, \"unk\") for idx in sequence] for sequence in x_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtlBNYZqPVKq"
      },
      "outputs": [],
      "source": [
        "embedding_model = api.load(\"glove-twitter-50\")\n",
        "embedding_size = embedding_model.vector_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhTZB87gaRML"
      },
      "outputs": [],
      "source": [
        "def embed_sequences(sequences, embedding_model, maxlen, embedding_size):\n",
        "    embeddings = []\n",
        "    for sequence in sequences:\n",
        "        seq_embedding = [\n",
        "            embedding_model[word] if word in embedding_model else np.zeros(embedding_size)\n",
        "            for word in sequence\n",
        "        ]\n",
        "        if len(seq_embedding) < maxlen:\n",
        "            # Padding with zero vectors if sequence is shorter\n",
        "            seq_embedding += [np.zeros(embedding_size)] * (maxlen - len(seq_embedding))\n",
        "        if len(seq_embedding) > maxlen:\n",
        "            seq_embedding = seq_embedding[:maxlen]\n",
        "\n",
        "        embeddings.append(seq_embedding)\n",
        "    return np.array(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyCuVKguaTMG"
      },
      "outputs": [],
      "source": [
        "x_train_embeddings = embed_sequences(x_train_words, embedding_model, maxlen, embedding_size)\n",
        "y_train_outputs = x_train_one_hot_encoded\n",
        "\n",
        "# Get correct values for next word prediction\n",
        "x_train_embeddings = x_train_embeddings[:, :maxlen - 1, :]\n",
        "y_train_outputs = y_train_outputs[:, -(maxlen - 1):, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBXa5NZ60uuf"
      },
      "source": [
        "#### Softmax Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnRfxzrNkR2m"
      },
      "outputs": [],
      "source": [
        "def softmax(x: np.ndarray) -> np.ndarray:\n",
        "    return np.exp(x) / np.sum(np.exp(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-5iuTIu4ucG"
      },
      "source": [
        "#### Input Layer Class\n",
        "\n",
        "Define the InputLayer class. The input player includes the following variables:   This variable represents the weight matrix connecting the input layer to the hidden layer. The dimensions are defined by the hidden size and the size of the input at each time step. It is initialized with small random values to facilitate learning during training.\n",
        "\n",
        "Variables:\n",
        "- `inputs`: The input sequences for the RNN, which are encoded using word2vec. It has a shape of (max_sequence_length, word2vec_size).\n",
        "- `weights`: This weight matrix connecting the input layer to the hidden layer.\n",
        "- `delta_weights`: The accumulation of the weight matrix gradients calculated across timesteps during backpropogation.\n",
        "\n",
        "You must implement the following functions:\n",
        "- `forward`: Multiply the input by the weight matrix. The output is a weighted sum that will be passed to the hidden layer for further processing.\n",
        "\n",
        "- `backward`: Calculate the gradient of the forward operation and add that value to `self.delta_weights`, to be updated in `update_parameters`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRgpALOwaUm_"
      },
      "outputs": [],
      "source": [
        "class InputLayer:\n",
        "    inputs: np.ndarray\n",
        "    weights: np.ndarray = None\n",
        "    delta_weights: np.ndarray = None\n",
        "\n",
        "    def __init__(self, inputs: np.ndarray, hidden_size: int) -> None:\n",
        "        self.inputs = inputs\n",
        "\n",
        "        limit = np.sqrt(6 / (len(inputs[0]) + hidden_size))\n",
        "        self.weights = np.random.uniform(low=-limit, high=limit, size=(hidden_size, len(inputs[0])))\n",
        "\n",
        "        self.delta_weights = np.zeros_like(self.weights)\n",
        "\n",
        "    def __reset_deltas__(self):\n",
        "        self.delta_weights = np.zeros_like(self.weights)\n",
        "\n",
        "    def get_input(self, time_step: int) -> np.ndarray:\n",
        "        return self.inputs[time_step][:, np.newaxis]\n",
        "\n",
        "    def forward(self, time_step: int) -> np.ndarray:\n",
        "        return self.weights @ self.get_input(time_step)\n",
        "\n",
        "    def backward(\n",
        "        self, time_step: int, delta_weights: np.ndarray\n",
        "    ) -> None:\n",
        "\n",
        "        self.delta_weights +=  np.dot(delta_weights, self.get_input(time_step).T)\n",
        "        # self.delta_weights += delta_weights\n",
        "\n",
        "    def update_parameters(self, learning_rate: float) -> None:\n",
        "        self.weights -= learning_rate * self.delta_weights\n",
        "\n",
        "        self.__reset_deltas__()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQFvMItcKi_5"
      },
      "source": [
        "#### Hidden Layer Class\n",
        "\n",
        "Define the HiddenLayer class. The hidden layer maintains the hidden states across time steps and computes the activations based on the weighted sum of the inputs, computed in the input layer, and the previous hidden state.\n",
        "\n",
        "variables:\n",
        "- `states`: The hidden states for all time steps during the sequence processing. It has a shape of (max_num_time_steps, hidden_size, 1).\n",
        "\n",
        "- `weights`: The weight matrix connecting the previous hidden layer.\n",
        "\n",
        "- `bias`: The bias vector added to the weighted sum of inputs and hidden states.\n",
        "\n",
        "- `delta_weights`: The accumulation of gradients of the weight matrix computed during backpropogation.\n",
        "\n",
        "- `delta_bias`: The accumulation of gradients of the bias vector computed during backpropogation.\n",
        "\n",
        "- `next_delta_hidden_state_activation`: The gradient of the previously calculated hidden state activation.\n",
        "\n",
        "\n",
        "You must implement the following functions:\n",
        "\n",
        "- `forward`: This function computes the forward pass through the hidden layer. It combines the weighted input, the previous hidden state, and the bias, followed by applying the hyperbolic tangent activation function (tanh). The resulting activation is stored and returned for further processing. Be sure to update the hidden state using `set_hidden_state`.\n",
        "\n",
        "- `backward`: This function computes the gradients for backpropagation. It calculates the delta for the activation using the incoming delta_output, propagates the gradients back to the previous hidden state, and updates delta_weights and delta_bias based on the computed gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwTWFPu0au0u"
      },
      "outputs": [],
      "source": [
        "class HiddenLayer:\n",
        "    states: np.ndarray = None\n",
        "    weights: np.ndarray = None\n",
        "    delta_weights: np.ndarray = None\n",
        "    bias: np.ndarray = None\n",
        "    delta_bias: np.ndarray = None\n",
        "    next_delta_hidden_state_activation: np.ndarray = None\n",
        "\n",
        "    def __init__(self, max_num_time_steps: int, size: int) -> None:\n",
        "        limit = np.sqrt(6 / (size + size))\n",
        "        self.weights = np.random.uniform(low=-limit, high=limit, size=(size, size))\n",
        "\n",
        "        self.bias = np.random.uniform(low=-0.1, high=0.1, size=(size, 1))\n",
        "        self.states = np.zeros(shape=(max_num_time_steps, size, 1))\n",
        "        self.next_delta_hidden_state_activation = np.zeros(shape=(size, 1))\n",
        "        self.delta_bias = np.zeros_like(self.bias)\n",
        "        self.delta_weights = np.zeros_like(self.weights)\n",
        "\n",
        "    def __reset_deltas__(self):\n",
        "        self.delta_bias = np.zeros_like(self.bias)\n",
        "        self.delta_weights = np.zeros_like(self.weights)\n",
        "        self.next_delta_hidden_state_activation = np.zeros_like(self.next_delta_hidden_state_activation)\n",
        "\n",
        "    def get_hidden_state(self, time_step: int) -> np.ndarray:\n",
        "            if time_step < 0:\n",
        "                return np.zeros_like(self.states[0])\n",
        "            return self.states[time_step]\n",
        "\n",
        "    def set_state(self, time_step: int, prediction: np.ndarray) -> None:\n",
        "        self.states[time_step] = prediction\n",
        "\n",
        "    def forward(self, weighted_input: np.ndarray, time_step: int) -> np.ndarray:\n",
        "        a_t = self.bias + np.dot(self.weights, self.get_hidden_state(time_step - 1)) + weighted_input\n",
        "        hidden_state = np.tanh(a_t)\n",
        "        self.set_state(time_step, hidden_state)\n",
        "        return hidden_state\n",
        "\n",
        "    def backward(\n",
        "        self, time_step: int, delta_output: np.ndarray\n",
        "    ) -> np.ndarray:\n",
        "        delta_hidden_state = delta_output + self.next_delta_hidden_state_activation\n",
        "        delta_weighted_sum = delta_hidden_state * (1 - (self.states[time_step]) ** 2)\n",
        "        self.next_delta_hidden_state_activation = np.dot(self.weights.T, delta_weighted_sum)\n",
        "        self.delta_weights += np.dot(delta_weighted_sum, self.get_hidden_state(time_step - 1).T)\n",
        "        self.delta_bias += delta_weighted_sum\n",
        "        return delta_weighted_sum\n",
        "\n",
        "    def update_parameters(self, learning_rate: float) -> None:\n",
        "        self.weights -= learning_rate * self.delta_weights\n",
        "        self.bias -= learning_rate * self.delta_bias\n",
        "\n",
        "        self.__reset_deltas__()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzZ0uRP5O-S9"
      },
      "source": [
        "#### Output Layer Class\n",
        "\n",
        "Define the OutputLayer class. The output layer will generate a probability distribution from the hidden state.\n",
        "\n",
        "Variables:\n",
        "- `states`: The computed probability distributions across all timesteps.\n",
        "\n",
        "- `weights`: The weight matrix connecting the hidden layer.\n",
        "\n",
        "- `bias`: The bias vector added to the output of the weighted sum.\n",
        "\n",
        "- `delta_weights`: The accumulation of gradients of the weight matrix computed during backpropogation.\n",
        "\n",
        "- `delta_bias`: The accumulation of gradients of the bias vector computed during backpropogation.\n",
        "\n",
        "You must implement the following functions:\n",
        "- `forward`: This function computes the forward pass through the output layer. It multiplies the hidden state by the weight matrix, adds the bias, and applies the softmax function to produce a probability distribution over the vocabulary. The resulting predictions MUST be stored using `set_prediction`.\n",
        "\n",
        "- `backward`: This function computes the gradients for backpropagation based on the expected outputs, updates delta_V and delta_bias with the computed gradients, and returns the propagated error to the previous layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SfYV3ircDlq"
      },
      "outputs": [],
      "source": [
        "class OutputLayer:\n",
        "    predictions: np.ndarray = None\n",
        "    weights: np.ndarray = None\n",
        "    bias: np.ndarray = None\n",
        "    delta_bias: np.ndarray = None\n",
        "    delta_weights: np.ndarray = None\n",
        "\n",
        "    def __init__(self, max_num_time_steps: int, size: int, hidden_size: int) -> None:\n",
        "        limit = np.sqrt(6 / (size + hidden_size))\n",
        "        self.weights = np.random.uniform(low=-limit, high=limit, size=(size, hidden_size))\n",
        "        self.bias = np.random.uniform(low=-0.1, high=0.1, size=(size, 1))\n",
        "        self.predictions = np.zeros(shape=(max_num_time_steps, size, 1))\n",
        "        self.delta_bias = np.zeros_like(self.bias)\n",
        "        self.delta_weights = np.zeros_like(self.weights)\n",
        "\n",
        "    def __reset_predictions__(self):\n",
        "        self.predictions = np.zeros_like(self.predictions)\n",
        "\n",
        "    def __reset_deltas__(self):\n",
        "        self.delta_bias = np.zeros_like(self.bias)\n",
        "        self.delta_weights = np.zeros_like(self.weights)\n",
        "\n",
        "    def forward(self, hidden_state: np.ndarray, time_step: int) -> np.ndarray:\n",
        "        self.predictions[time_step] = softmax(self.bias + np.dot(self.weights, hidden_state))\n",
        "        self.set_prediction(time_step, self.predictions[time_step])\n",
        "        return self.predictions[time_step]\n",
        "\n",
        "    def get_prediction(self, time_step: int) -> np.ndarray:\n",
        "        return self.predictions[time_step]\n",
        "\n",
        "    def set_prediction(self, time_step: int, prediction: np.ndarray) -> None:\n",
        "        self.predictions[time_step] = prediction\n",
        "\n",
        "    def backward(\n",
        "        self,\n",
        "        expected: np.ndarray,\n",
        "        hidden_state: np.ndarray,\n",
        "        time_step: int,\n",
        "    ) -> np.ndarray:\n",
        "        prediction = self.get_prediction(time_step)\n",
        "        expected = expected.reshape(-1, 1)\n",
        "        delta_output = prediction - expected\n",
        "        self.delta_weights += delta_output @ hidden_state.T\n",
        "\n",
        "        self.delta_bias += delta_output\n",
        "        return self.delta_weights.T @ delta_output\n",
        "\n",
        "\n",
        "    def update_parameters(self, learning_rate: float) -> None:\n",
        "        self.weights -= learning_rate * self.delta_weights\n",
        "        self.bias -= learning_rate * self.delta_bias\n",
        "\n",
        "        self.__reset_deltas__()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "li17DVjCCYUG"
      },
      "source": [
        "#### RNN Class\n",
        "The RNN class is a Recurrent Neural Network that combines the input, hidden, and output layers to process sequences of inputs, learn their representations, and generate predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjwSh87BelhP"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "class RNN:\n",
        "    hidden_layer: HiddenLayer\n",
        "    output_layer: OutputLayer\n",
        "    learning_rate: float\n",
        "    input_layer: InputLayer = None\n",
        "\n",
        "    def __init__(self, vocab_size: int, hidden_size: int, max_num_time_steps: int, learning_rate: float) -> None:\n",
        "        self.hidden_layer = HiddenLayer(max_num_time_steps, hidden_size)\n",
        "        self.output_layer = OutputLayer(max_num_time_steps, vocab_size, hidden_size)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def feed_forward(self, inputs: np.ndarray) -> OutputLayer:\n",
        "        self.input_layer = InputLayer(inputs, self.hidden_size)\n",
        "\n",
        "        for step in range(len(inputs)):\n",
        "            weighted_input = self.input_layer.forward(step)\n",
        "            activation = self.hidden_layer.forward(weighted_input, step)\n",
        "            self.output_layer.forward(activation, step)\n",
        "\n",
        "        return self.output_layer\n",
        "\n",
        "    def backpropagation(self, expected: np.ndarray) -> None:\n",
        "        for step_number in reversed(range(len(expected) - 1)):\n",
        "            delta_output = self.output_layer.backward(\n",
        "                expected[step_number],\n",
        "                self.hidden_layer.get_hidden_state(step_number),\n",
        "                step_number,\n",
        "            )\n",
        "            delta_weighted_sum = self.hidden_layer.backward(\n",
        "                step_number, delta_output\n",
        "            )\n",
        "            self.input_layer.backward(step_number, delta_weighted_sum)\n",
        "\n",
        "\n",
        "        self.output_layer.update_parameters(self.learning_rate)\n",
        "        self.hidden_layer.update_parameters(self.learning_rate)\n",
        "        self.input_layer.update_parameters(self.learning_rate)\n",
        "\n",
        "    def loss(self, y_hat: List[np.ndarray], y: List[np.ndarray]) -> float:\n",
        "        return -np.mean([np.sum(y[i] * np.log(y_hat[i])) for i in range(len(y))])\n",
        "\n",
        "    def _find_end_of_seq(self, expected: np.ndarray) -> int:\n",
        "        for idx, vector in enumerate(expected):\n",
        "            if np.all(vector == 0):\n",
        "                return idx\n",
        "        return len(expected)\n",
        "\n",
        "    def _reset_states(self):\n",
        "      self.output_layer.__reset_predictions__()\n",
        "      self.hidden_layer.__reset_deltas__()\n",
        "\n",
        "    def train(self, inputs: np.ndarray, expected: np.ndarray, epochs: int) -> None:\n",
        "        for epoch in range(epochs):\n",
        "            loss_list = []\n",
        "            for idx, input in enumerate(tqdm(inputs)):\n",
        "                end_idx = self._find_end_of_seq(expected[idx])\n",
        "                input = input[:end_idx, :]\n",
        "\n",
        "                y_hats = self.feed_forward(input)\n",
        "                self.backpropagation(expected[idx][:end_idx])\n",
        "\n",
        "                round_loss = self.loss(y_hats.predictions[:end_idx,:,0], expected[idx][:end_idx])\n",
        "\n",
        "                loss_list.append(round_loss)\n",
        "\n",
        "                self._reset_states()\n",
        "\n",
        "                if idx % 100 == 99:\n",
        "                  print(f\"Average Training Loss of Last 100 samples: {np.mean(np.array(loss_list[-100:]))}\")\n",
        "\n",
        "            print(\n",
        "                f\"Epoch Loss: {np.mean(np.array(loss_list))}\"\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tp8Grw7erxk",
        "outputId": "17d47ba3-d95a-4b34-fffc-7858aa2d5545"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▎         | 101/2773 [00:04<02:36, 17.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 6.78097193174175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 204/2773 [00:09<01:15, 34.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 6.458141142079012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 11%|█         | 305/2773 [00:12<01:09, 35.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 6.144313403071155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|█▍        | 404/2773 [00:15<01:05, 36.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.963844879921611\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 18%|█▊        | 502/2773 [00:18<02:03, 18.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.810792623853397\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 22%|██▏       | 606/2773 [00:23<01:04, 33.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.688416969047098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 704/2773 [00:26<01:02, 33.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.677526396047721\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 29%|██▉       | 804/2773 [00:29<00:53, 36.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.628703472657036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|███▎      | 905/2773 [00:37<01:23, 22.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.582881600674271\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 36%|███▋      | 1006/2773 [00:40<00:57, 30.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.634657779985662\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|███▉      | 1105/2773 [00:43<00:50, 33.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.567767807090373\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 43%|████▎     | 1206/2773 [00:46<00:44, 35.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.515945507417717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 47%|████▋     | 1305/2773 [00:52<00:53, 27.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.48925390939297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 51%|█████     | 1403/2773 [00:55<00:44, 30.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.451904780706798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|█████▍    | 1506/2773 [00:58<00:37, 33.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.382791965791131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 58%|█████▊    | 1605/2773 [01:01<00:36, 32.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.346290695546867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 61%|██████▏   | 1704/2773 [01:07<00:41, 26.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.407473329339945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▌   | 1805/2773 [01:10<00:30, 31.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.4190917666164555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 69%|██████▊   | 1906/2773 [01:13<00:26, 32.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.271562605877867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 72%|███████▏  | 2004/2773 [01:16<00:22, 33.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.320541288232228\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 76%|███████▌  | 2104/2773 [01:22<00:20, 31.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.285571084257276\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|███████▉  | 2205/2773 [01:25<00:17, 32.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.19970310202633\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 83%|████████▎ | 2305/2773 [01:28<00:16, 29.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.306533433339752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 87%|████████▋ | 2403/2773 [01:31<00:11, 32.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.209383347031619\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 2503/2773 [01:37<00:09, 27.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.179569591179846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 94%|█████████▍| 2603/2773 [01:40<00:05, 30.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.278857830611155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 98%|█████████▊| 2707/2773 [01:45<00:01, 33.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.273460238221945\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2773/2773 [01:49<00:00, 25.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch Loss: 5.556007228611861\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▎         | 103/2773 [00:04<01:19, 33.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.232424199028145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 204/2773 [00:07<01:21, 31.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.159650819262527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 11%|█         | 304/2773 [00:10<01:12, 34.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.153900926836169\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|█▍        | 403/2773 [00:14<02:28, 15.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.173566833439274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 18%|█▊        | 506/2773 [00:19<01:03, 35.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.123117350291804\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 22%|██▏       | 603/2773 [00:22<01:05, 33.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.057981297745074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 706/2773 [00:25<01:04, 31.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.114141358055605\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 29%|██▉       | 800/2773 [00:29<01:38, 20.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.124243433086153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|███▎      | 905/2773 [00:33<01:00, 31.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.113243355446401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 36%|███▋      | 1006/2773 [00:36<00:57, 30.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.179696489842646\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|███▉      | 1103/2773 [00:39<00:48, 34.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.155986160685412\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 43%|████▎     | 1203/2773 [00:44<01:33, 16.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.11855976113497\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 47%|████▋     | 1306/2773 [00:48<00:40, 36.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.106609329938137\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 51%|█████     | 1406/2773 [00:52<00:43, 31.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.106375362500284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|█████▍    | 1504/2773 [00:55<00:40, 31.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.061463760508717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 58%|█████▊    | 1602/2773 [01:00<01:36, 12.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.06762850068052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 61%|██████▏   | 1704/2773 [01:03<00:33, 31.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.143132921562344\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▌   | 1803/2773 [01:07<00:29, 32.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.158912470551868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 69%|██████▊   | 1903/2773 [01:10<00:27, 31.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.0445908382555205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 72%|███████▏  | 2001/2773 [01:15<00:44, 17.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.093860786549579\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 76%|███████▌  | 2107/2773 [01:19<00:18, 35.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.054944063388456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|███████▉  | 2206/2773 [01:22<00:17, 32.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 4.99733303679714\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 83%|████████▎ | 2304/2773 [01:25<00:16, 28.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.116718582878943\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 87%|████████▋ | 2403/2773 [01:30<00:22, 16.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.028436081556414\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 2504/2773 [01:34<00:08, 30.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.0194488609748875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 94%|█████████▍| 2604/2773 [01:37<00:05, 32.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.114650610889404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 97%|█████████▋| 2703/2773 [01:40<00:02, 28.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss of Last 100 samples: 5.124908575025793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2773/2773 [01:43<00:00, 26.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch Loss: 5.107838363479491\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "rnn = RNN(vocab_size=vocab_size, hidden_size=32, max_num_time_steps=maxlen - 1, learning_rate=1e-3)\n",
        "rnn.train(x_train_embeddings, y_train_outputs, epochs=2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}